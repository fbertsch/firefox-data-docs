<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>In-depth AWS Data Pipeline Detail - Mozilla Data Documentation</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="../../favicon.svg">
        
        
        <link rel="shortcut icon" href="../../favicon.png">
        
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        
        <link rel="stylesheet" href="../../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="../../dtmo.css">
        
        <link rel="stylesheet" href="../../mermaid.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../introduction.html">Mozilla Data Documentation</a></li><li class="chapter-item expanded "><a href="../../concepts/getting_started.html"><strong aria-hidden="true">1.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../concepts/terminology.html"><strong aria-hidden="true">1.1.</strong> Terminology</a></li><li class="chapter-item expanded "><a href="../../concepts/gaining_access.html"><strong aria-hidden="true">1.2.</strong> Gaining Access</a></li><li class="chapter-item expanded "><a href="../../concepts/analysis_intro.html"><strong aria-hidden="true">1.3.</strong> Analysis Quick Start</a></li><li class="chapter-item expanded "><a href="../../tools/interfaces.html"><strong aria-hidden="true">1.4.</strong> Tools for Data Analysis</a></li><li class="chapter-item expanded "><a href="../../concepts/analysis_gotchas.html"><strong aria-hidden="true">1.5.</strong> Common Analysis Gotchas</a></li><li class="chapter-item expanded "><a href="../../concepts/choosing_a_dataset.html"><strong aria-hidden="true">1.6.</strong> Choosing a Desktop Dataset</a></li><li class="chapter-item expanded "><a href="../../concepts/choosing_a_dataset_mobile.html"><strong aria-hidden="true">1.7.</strong> Choosing a Mobile Dataset</a></li><li class="chapter-item expanded "><a href="../../concepts/getting_help.html"><strong aria-hidden="true">1.8.</strong> Getting Help</a></li><li class="chapter-item expanded "><a href="../../concepts/reporting_a_problem.html"><strong aria-hidden="true">1.9.</strong> Reporting a problem</a></li></ol></li><li class="chapter-item expanded "><a href="../../metrics/index.html"><strong aria-hidden="true">2.</strong> Metrics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../metrics/definitions.html"><strong aria-hidden="true">2.1.</strong> Definitions</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../metrics/metrics.html"><strong aria-hidden="true">2.1.1.</strong> Metrics</a></li><li class="chapter-item expanded "><a href="../../metrics/usage.html"><strong aria-hidden="true">2.1.2.</strong> Usage Criteria</a></li><li class="chapter-item expanded "><a href="../../metrics/dimensions.html"><strong aria-hidden="true">2.1.3.</strong> Slicing Dimensions</a></li></ol></li><li class="chapter-item expanded "><a href="../../metrics/policy.html"><strong aria-hidden="true">2.2.</strong> Metrics Standardization and Policy</a></li></ol></li><li class="chapter-item expanded "><a href="../../cookbooks/index.html"><strong aria-hidden="true">3.</strong> Tutorials &amp; Cookbooks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/analysis/index.html"><strong aria-hidden="true">3.1.</strong> Analysis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/public_data.html"><strong aria-hidden="true">3.1.1.</strong> Accessing Public Data</a></li><li class="chapter-item expanded "><a href="../../cookbooks/bigquery.html"><strong aria-hidden="true">3.1.2.</strong> Accessing and working with BigQuery</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/bigquery/access.html"><strong aria-hidden="true">3.1.2.1.</strong> Access</a></li><li class="chapter-item expanded "><a href="../../cookbooks/bigquery/querying.html"><strong aria-hidden="true">3.1.2.2.</strong> Writing Queries</a></li><li class="chapter-item expanded "><a href="../../cookbooks/bigquery/optimization.html"><strong aria-hidden="true">3.1.2.3.</strong> Optimization</a></li><li class="chapter-item expanded "><a href="../../tools/spark.html"><strong aria-hidden="true">3.1.2.4.</strong> Custom analysis with Spark</a></li></ol></li><li class="chapter-item expanded "><a href="../../tools/stmo.html"><strong aria-hidden="true">3.1.3.</strong> Introduction to STMO</a></li><li class="chapter-item expanded "><a href="../../concepts/glean/accessing_glean_data.html"><strong aria-hidden="true">3.1.4.</strong> Accessing Glean data</a></li><li class="chapter-item expanded "><a href="../../cookbooks/dataset_specific.html"><strong aria-hidden="true">3.1.5.</strong> Dataset-Specific</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/normandy_events.html"><strong aria-hidden="true">3.1.5.1.</strong> Working with Normandy events</a></li><li class="chapter-item expanded "><a href="../../cookbooks/crash_pings.html"><strong aria-hidden="true">3.1.5.2.</strong> Working with Crash Pings</a></li><li class="chapter-item expanded "><a href="../../cookbooks/clients_last_seen_bits.html"><strong aria-hidden="true">3.1.5.3.</strong> Working with Bit Patterns in Clients Last Seen</a></li><li class="chapter-item expanded "><a href="../../cookbooks/main_ping_exponential_histograms.html"><strong aria-hidden="true">3.1.5.4.</strong> Visualizing Percentiles of an Main Ping Exponential Histogram</a></li></ol></li><li class="chapter-item expanded "><a href="../../cookbooks/realtime.html"><strong aria-hidden="true">3.1.6.</strong> Real-time</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/view_pings_cep.html"><strong aria-hidden="true">3.1.6.1.</strong> Seeing Your Own Pings</a></li></ol></li><li class="chapter-item expanded "><a href="../../cookbooks/metrics.html"><strong aria-hidden="true">3.1.7.</strong> Metrics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/dau.html"><strong aria-hidden="true">3.1.7.1.</strong> Daily Active Users (DAU and MAU)</a></li><li class="chapter-item expanded "><a href="../../cookbooks/active_dau.html"><strong aria-hidden="true">3.1.7.2.</strong> Active DAU (aDAU)</a></li><li class="chapter-item expanded "><a href="../../cookbooks/retention.html"><strong aria-hidden="true">3.1.7.3.</strong> Retention</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../cookbooks/operational/index.html"><strong aria-hidden="true">3.2.</strong> Operational</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/gcp-projects.html"><strong aria-hidden="true">3.2.1.</strong> Creating a Prototype Data Project on Google Cloud Platform</a></li><li class="chapter-item expanded "><a href="../../cookbooks/operational/protosaur.html"><strong aria-hidden="true">3.2.2.</strong> Creating Static Dashboards with Protosaur</a></li><li class="chapter-item expanded "><a href="../../cookbooks/bigquery-airflow.html"><strong aria-hidden="true">3.2.3.</strong> Scheduling BigQuery Queries in Airflow</a></li><li class="chapter-item expanded "><a href="../../cookbooks/deploying-containers.html"><strong aria-hidden="true">3.2.4.</strong> Building and Deploying Containers to GCR with CircleCI</a></li><li class="chapter-item expanded "><a href="../../cookbooks/publishing_datasets.html"><strong aria-hidden="true">3.2.5.</strong> Publishing Datasets</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/new_data.html"><strong aria-hidden="true">3.3.</strong> Sending telemetry</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../cookbooks/client_guidelines.html"><strong aria-hidden="true">3.3.1.</strong> Implementing Experiments</a></li><li class="chapter-item expanded "><a href="../../cookbooks/events_best_practices.html"><strong aria-hidden="true">3.3.2.</strong> Sending Events</a></li><li class="chapter-item expanded "><a href="../../cookbooks/new_ping.html"><strong aria-hidden="true">3.3.3.</strong> Sending a Custom Ping</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../reference/index.html"><strong aria-hidden="true">4.</strong> Data Platform Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../tools/guiding_principles.html"><strong aria-hidden="true">4.1.</strong> Guiding Principles for Data Infrastructure</a></li><li class="chapter-item expanded "><a href="../../concepts/glean/glean.html"><strong aria-hidden="true">4.2.</strong> Glean overview</a></li><li class="chapter-item expanded "><a href="../../concepts/pipeline/gcp_data_pipeline.html"><strong aria-hidden="true">4.3.</strong> Overview of Mozilla's Data Pipeline</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../concepts/pipeline/http_edge_spec.html"><strong aria-hidden="true">4.3.1.</strong> HTTP Edge Server Specification</a></li><li class="chapter-item expanded "><a href="../../concepts/pipeline/event_pipeline.html"><strong aria-hidden="true">4.3.2.</strong> Event Pipeline Detail</a></li><li class="chapter-item expanded "><a href="../../concepts/pipeline/schemas.html"><strong aria-hidden="true">4.3.3.</strong> Schemas</a></li><li class="chapter-item expanded "><a href="../../concepts/channels/channel_normalization.html"><strong aria-hidden="true">4.3.4.</strong> Channel Normalization</a></li><li class="chapter-item expanded "><a href="../../concepts/sample_id.html"><strong aria-hidden="true">4.3.5.</strong> Sampling</a></li><li class="chapter-item expanded "><a href="../../concepts/pipeline/filtering.html"><strong aria-hidden="true">4.3.6.</strong> Filtering</a></li></ol></li><li class="chapter-item expanded "><a href="../../concepts/sql_style.html"><strong aria-hidden="true">4.4.</strong> SQL Style Guide</a></li><li class="chapter-item expanded "><a href="../../concepts/index.html"><strong aria-hidden="true">4.5.</strong> Telemetry Behavior Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../concepts/history.html"><strong aria-hidden="true">4.5.1.</strong> History of Telemetry</a></li><li class="chapter-item expanded "><a href="../../concepts/profile/index.html"><strong aria-hidden="true">4.5.2.</strong> Profile Behavior</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../concepts/profile/profile_creation.html"><strong aria-hidden="true">4.5.2.1.</strong> Profile Creation</a></li><li class="chapter-item expanded "><a href="../../concepts/profile/realworldusage.html"><strong aria-hidden="true">4.5.2.2.</strong> Real World Usage</a></li><li class="chapter-item expanded "><a href="../../concepts/profile/profilehistory.html"><strong aria-hidden="true">4.5.2.3.</strong> Profile History</a></li></ol></li><li class="chapter-item expanded "><a href="../../concepts/engagement.html"><strong aria-hidden="true">4.5.3.</strong> Engagement metrics</a></li><li class="chapter-item expanded "><a href="../../concepts/segments.html"><strong aria-hidden="true">4.5.4.</strong> User states/Segments</a></li></ol></li><li class="chapter-item expanded "><a href="../../tools/projects.html"><strong aria-hidden="true">4.6.</strong> Project Glossary</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/reference.html"><strong aria-hidden="true">5.</strong> Dataset Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/pings.html"><strong aria-hidden="true">5.1.</strong> Pings</a></li><li class="chapter-item expanded "><a href="../../datasets/derived.html"><strong aria-hidden="true">5.2.</strong> Derived Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/active_profiles.html"><strong aria-hidden="true">5.2.1.</strong> Active Profiles</a></li><li class="chapter-item expanded "><a href="../../datasets/batch_view/addons/reference.html"><strong aria-hidden="true">5.2.2.</strong> Addons</a></li><li class="chapter-item expanded "><a href="../../datasets/other/addons_daily/reference.html"><strong aria-hidden="true">5.2.3.</strong> Addons Daily</a></li><li class="chapter-item expanded "><a href="../../datasets/other/asn_aggregates/reference.html"><strong aria-hidden="true">5.2.4.</strong> Autonomous System Aggregates</a></li><li class="chapter-item expanded "><a href="../../datasets/other/attitudes_daily/reference.html"><strong aria-hidden="true">5.2.5.</strong> Attitudes Daily</a></li><li class="chapter-item expanded "><a href="../../datasets/batch_view/clients_daily/reference.html"><strong aria-hidden="true">5.2.6.</strong> Clients Daily</a></li><li class="chapter-item expanded "><a href="../../datasets/bigquery/clients_last_seen/reference.html"><strong aria-hidden="true">5.2.7.</strong> Clients Last Seen</a></li><li class="chapter-item expanded "><a href="../../datasets/batch_view/events/reference.html"><strong aria-hidden="true">5.2.8.</strong> Events</a></li><li class="chapter-item expanded "><a href="../../datasets/bigquery/exact_mau/reference.html"><strong aria-hidden="true">5.2.9.</strong> Exact MAU</a></li><li class="chapter-item expanded "><a href="../../datasets/main_ping_tables.html"><strong aria-hidden="true">5.2.10.</strong> Main Ping Tables</a></li><li class="chapter-item expanded "><a href="../../datasets/batch_view/main_summary/reference.html"><strong aria-hidden="true">5.2.11.</strong> Main Summary</a></li><li class="chapter-item expanded "><a href="../../datasets/other/socorro_crash/reference.html"><strong aria-hidden="true">5.2.12.</strong> Socorro Crash Reports</a></li><li class="chapter-item expanded "><a href="../../datasets/other/ssl/reference.html"><strong aria-hidden="true">5.2.13.</strong> SSL Ratios (public)</a></li><li class="chapter-item expanded "><a href="../../datasets/batch_view/telemetry_aggregates/reference.html"><strong aria-hidden="true">5.2.14.</strong> Telemetry Aggregates</a></li></ol></li><li class="chapter-item expanded "><a href="../../tools/experiments.html"><strong aria-hidden="true">5.3.</strong> Experiment Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/jetstream.html"><strong aria-hidden="true">5.3.1.</strong> Jetstream</a></li><li class="chapter-item expanded "><a href="../../datasets/heartbeat.html"><strong aria-hidden="true">5.3.2.</strong> Accessing Heartbeat data</a></li><li class="chapter-item expanded "><a href="../../datasets/shield.html"><strong aria-hidden="true">5.3.3.</strong> Accessing Shield Study data</a></li><li class="chapter-item expanded "><a href="../../datasets/dynamic_telemetry.html"><strong aria-hidden="true">5.3.4.</strong> Dynamic telemetry</a></li><li class="chapter-item expanded "><a href="../../datasets/experiment_monitoring.html"><strong aria-hidden="true">5.3.5.</strong> Experiment monitoring</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/search.html"><strong aria-hidden="true">5.4.</strong> Search Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/search/search_aggregates/reference.html"><strong aria-hidden="true">5.4.1.</strong> Search Aggregates</a></li><li class="chapter-item expanded "><a href="../../datasets/search/search_clients_daily/reference.html"><strong aria-hidden="true">5.4.2.</strong> Search Clients Daily</a></li><li class="chapter-item expanded "><a href="../../datasets/search/search_clients_last_seen/reference.html"><strong aria-hidden="true">5.4.3.</strong> Search Clients Last Seen</a></li><li class="chapter-item expanded "><a href="../../datasets/search/client_ltv/reference.html"><strong aria-hidden="true">5.4.4.</strong> Client LTV</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/non_desktop.html"><strong aria-hidden="true">5.5.</strong> Non-Desktop Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/non_desktop/day_2_7_activation/reference.html"><strong aria-hidden="true">5.5.1.</strong> Day 2-7 Activation</a></li><li class="chapter-item expanded "><a href="../../datasets/non_desktop/google_play_store/reference.html"><strong aria-hidden="true">5.5.2.</strong> Google Play Store</a></li><li class="chapter-item expanded "><a href="../../datasets/non_desktop/apple_app_store/reference.html"><strong aria-hidden="true">5.5.3.</strong> Apple App Store</a></li><li class="chapter-item expanded "><a href="../../datasets/non_desktop/events_daily/reference.html"><strong aria-hidden="true">5.5.4.</strong> Events Daily</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/other.html"><strong aria-hidden="true">5.6.</strong> Other Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/other/hgpush/reference.html"><strong aria-hidden="true">5.6.1.</strong> hgpush</a></li><li class="chapter-item expanded "><a href="../../datasets/other/stub_installer/reference.html"><strong aria-hidden="true">5.6.2.</strong> Stub installer ping</a></li><li class="chapter-item expanded "><a href="../../datasets/other/activity-stream/reference.html"><strong aria-hidden="true">5.6.3.</strong> Activity Stream</a></li><li class="chapter-item expanded "><a href="../../datasets/other/bmobugs/reference.html"><strong aria-hidden="true">5.6.4.</strong> bmobugs</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/fxa.html"><strong aria-hidden="true">5.7.</strong> Firefox Accounts Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/fxa_metrics/attribution.html"><strong aria-hidden="true">5.7.1.</strong> Firefox Account Attribution</a></li><li class="chapter-item expanded "><a href="../../datasets/fxa_metrics/funnels.html"><strong aria-hidden="true">5.7.2.</strong> Firefox Account Funnel Metrics</a></li><li class="chapter-item expanded "><a href="../../datasets/fxa_metrics/emails.html"><strong aria-hidden="true">5.7.3.</strong> Firefox Account Email Metrics</a></li></ol></li><li class="chapter-item expanded "><a href="../../datasets/static.html"><strong aria-hidden="true">5.8.</strong> Static Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/static/normalized_os.html"><strong aria-hidden="true">5.8.1.</strong> Normalized OS Names And Versions</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../historical/index.html"><strong aria-hidden="true">6.</strong> Historical Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../concepts/pipeline/data_pipeline.html"><strong aria-hidden="true">6.1.</strong> Previous AWS Pipeline Overview</a></li><li class="chapter-item expanded "><a href="../../concepts/pipeline/data_pipeline_detail.html" class="active"><strong aria-hidden="true">6.2.</strong> In-depth AWS Data Pipeline Detail</a></li><li class="chapter-item expanded "><a href="../../concepts/censuses.html"><strong aria-hidden="true">6.3.</strong> Legacy Census Metrics</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete.html"><strong aria-hidden="true">6.4.</strong> Obsolete Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../datasets/obsolete/churn/reference.html"><strong aria-hidden="true">6.4.1.</strong> Churn</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/client_count/reference.html"><strong aria-hidden="true">6.4.2.</strong> Client Count</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/client_count_daily/reference.html"><strong aria-hidden="true">6.4.3.</strong> Client Count Daily</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/crash_aggregates/reference.html"><strong aria-hidden="true">6.4.4.</strong> Crash Aggregates</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/crash_summary/reference.html"><strong aria-hidden="true">6.4.5.</strong> Crash Summary</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/error_aggregates/reference.html"><strong aria-hidden="true">6.4.6.</strong> Error Aggregates</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/first_shutdown_summary/reference.html"><strong aria-hidden="true">6.4.7.</strong> First Shutdown Summary</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/heavy_users/reference.html"><strong aria-hidden="true">6.4.8.</strong> Heavy Users</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/longitudinal/reference.html"><strong aria-hidden="true">6.4.9.</strong> Longitudinal</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/retention/reference.html"><strong aria-hidden="true">6.4.10.</strong> Retention</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/sync_summary/reference.html"><strong aria-hidden="true">6.4.11.</strong> Sync Summary</a></li><li class="chapter-item expanded "><a href="../../datasets/obsolete/update/reference.html"><strong aria-hidden="true">6.4.12.</strong> Update</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../contributing/index.html"><strong aria-hidden="true">7.</strong> Contributing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../contributing/style_guide.html"><strong aria-hidden="true">7.1.</strong> Style Guide</a></li><li class="chapter-item expanded "><a href="../../contributing/structure.html"><strong aria-hidden="true">7.2.</strong> Structure</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Mozilla Data Documentation</h1>

                    <div class="right-buttons">
                        
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/mozilla/data-docs" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#a-detailed-look-at-the-data-platform" id="a-detailed-look-at-the-data-platform">A Detailed Look at the Data Platform</a></h1>
<p>For a more gentle introduction to the data platform, please read the <a href="data_pipeline.html">Pipeline Overview</a> article.</p>
<p>This article goes into more depth about the architecture and flow of data in the platform.</p>
<h2><a class="header" href="#the-entire-platform" id="the-entire-platform">The Entire Platform</a></h2>
<p>The full detail of the platform can get quite complex, but at a high level the structure is fairly simple.</p>
<pre class="mermaid">graph LR
  Producers[Data Producers] --&gt; Ingestion
  Ingestion --&gt; Storage[Long-term Storage]
  Ingestion --&gt; Stream[Stream Processing]
  Stream --&gt; Storage
  Batch[Batch Processing] --&gt; Storage
  Storage --&gt; Batch
  Self[Self Serve] -.- Stream
  Self -.- Batch
  Stream -.-&gt; Visualization
  Batch -.-&gt; Visualization
  Stream --&gt; Export
  Batch --&gt; Export
</pre>
<p>Each of these high-level parts of the platform are described in more detail below.</p>
<h2><a class="header" href="#data-producers" id="data-producers">Data Producers</a></h2>
<p>By far most data handled by the Data Platform is <a href="https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/data/main-ping.html">produced by Firefox</a>. There are other producers, though, and the eventual aim is to generalize data production using a client SDK or set of standard tools.</p>
<p>Most data is submitted via HTTP POST, but data is also produced in the form of service logs and <code>statsd</code> messages.</p>
<p>If you would like to locally test a new data producer, the <a href="https://github.com/mozilla/gzipServer"><code>gzipServer</code></a> project provides a simplified server that makes it easy to inspect submitted messages.</p>
<h2><a class="header" href="#ingestion" id="ingestion">Ingestion</a></h2>
<pre class="mermaid">graph LR
  subgraph HTTP
    tee
    lb[Load Balancer]
    mozingest
  end
  subgraph Kafka
    kafka_unvalidated[Kafka unvalidated]
    kafka_validated[Kafka validated]
    zookeeper[ZooKeeper] -.- kafka_unvalidated
    zookeeper -.- kafka_validated
  end
  subgraph Storage
    s3_heka[S3 Heka Protobuf Storage]
    s3_parquet[S3 Parquet Storage]
  end
  subgraph Data Producers
    Firefox --&gt; lb
    more_producers[Other Producers] --&gt; lb
  end

  lb --&gt; tee
  tee --&gt; mozingest
  mozingest --&gt; kafka_unvalidated
  mozingest --&gt; Landfill
  kafka_unvalidated --&gt; dwl[Data Store Loader]
  kafka_validated --&gt; cep[Hindsight CEP]
  kafka_validated --&gt; sparkstreaming[Spark Streaming]
  Schemas -.-&gt;|validation| dwl
  dwl --&gt; kafka_validated
  dwl --&gt; s3_heka
  dwl --&gt; s3_parquet
  sparkstreaming --&gt; s3_parquet
</pre>
<p>Data arrives as an HTTP POST of an optionally gzipped payload of JSON. See the common <a href="http_edge_spec.html">Edge Server</a> specification for details.</p>
<p>Submissions hit a load balancer which handles the SSL connection, then forwards to a &quot;tee&quot; server, which may direct some or all submissions to alternate backends. In the past, the tee was used to manage the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1302265">cutover between different versions of the backend</a> infrastructure. It is implemented as an <a href="http://openresty.org/en/"><code>OpenResty</code></a> plugin.</p>
<p>From there, the <a href="https://github.com/mozilla-services/nginx_moz_ingest"><code>mozingest</code></a> HTTP Server receives submissions from the tee and batches and stores data durably on Amazon S3 as a fail-safe (we call this &quot;Landfill&quot;). Data is then passed along via <a href="https://kafka.apache.org/">Kafka</a> for validation and further processing. If there is a problem with decoding, validation, or any of the code described in the rest of this section, data can be re-processed from this fail-safe store. The <code>mozingest</code> server is implemented as an <code>nginx</code> module.</p>
<p>Validation, at a minimum, ensures that a payload is valid JSON (possibly compressed). Many document types also have a <a href="https://github.com/mozilla-services/mozilla-pipeline-schemas">JSONSchema specification</a>, and are further validated against that.</p>
<p>Invalid messages are redirected to a separate &quot;errors&quot; stream for debugging and inspection.</p>
<p>Valid messages proceed for further decoding and processing. This involves things like doing GeoIP lookup and discarding the IP address, and attaching some HTTP header info as annotated metadata.</p>
<p>Validated and annotated messages become available for stream processing.</p>
<p>They are also batched and stored durably for later batch processing and ad-hoc querying.</p>
<p>See also the &quot;<a href="https://docs.google.com/document/d/1PqiF1rF2fCk_kQuGSwGwildDf4Crg9MJTY44E6N5DSk/edit">generic ingestion</a>&quot; proposal which aims to make ingestion, validation, storage, and querying available as self-serve for platform users.</p>
<h5><a class="header" href="#data-flow-for-valid-submissions" id="data-flow-for-valid-submissions">Data flow for valid submissions</a></h5>
<pre class="mermaid">sequenceDiagram
    participant Fx as Firefox
    participant lb as Load Balancer
    participant mi as mozingest
    participant lf as Landfill
    participant k as Kafka
    participant dwl as Data Store Loader
    participant dl as Data Lake

    Fx-&gt;&gt;lb: HTTPS POST
    lb-&gt;&gt;mi: forward
    mi--&gt;&gt;lf: failsafe store
    mi-&gt;&gt;k: enqueue
    k-&gt;&gt;dwl: validate, decode
    dwl-&gt;&gt;k: enqueue validated
    dwl-&gt;&gt;dl: store durably
</pre>
<h5><a class="header" href="#other-ingestion-methods" id="other-ingestion-methods">Other ingestion methods</a></h5>
<p>Hindsight is used for <a href="https://mozilla-services.github.io/lua_sandbox_extensions/moz_logging/">ingestion of logs</a> from applications and services, it supports parsing of log lines and appending similar metadata as the HTTP ingestion above (timestamp, source, and so on).</p>
<p><a href="https://github.com/etsy/statsd"><code>Statsd</code></a> messages are ingested in the usual way.</p>
<h2><a class="header" href="#storage" id="storage">Storage</a></h2>
<pre class="mermaid">graph TD
  subgraph RDBMS
    PostgreSQL
    Redshift
    MySQL
    BigQuery
  end
  subgraph NoSQL
    DynamoDB
  end
  subgraph S3
    landfill[Landfill]
    s3_heka[Heka Data Lake]
    s3_parquet[Parquet Data Lake]
    s3_analysis[Analysis Outputs]
    s3_public[Public Outputs]
  end

  Ingestion --&gt; s3_heka
  Ingestion --&gt; s3_parquet
  Ingestion --&gt; landfill
  Ingestion -.-&gt; stream[Stream Processing]
  stream --&gt; s3_parquet
  batch[Batch Processing] --&gt; s3_parquet
  batch --&gt; PostgreSQL
  batch --&gt; DynamoDB
  batch --&gt; s3_public
  selfserve[Self Serve] --&gt; s3_analysis
  s3_analysis --&gt; selfserve
  Hive --&gt;|Presto| STMO[STMO]
  PostgreSQL --&gt; STMO
  Redshift --&gt; STMO
  MySQL --&gt; STMO
  BigQuery --&gt; STMO

  s3_parquet -.- Hive
</pre>
<p><a href="https://aws.amazon.com/s3/">Amazon S3</a> forms the backbone of the platform storage layer. The primary format used in the Data Lake is <a href="https://parquet.apache.org/">parquet</a>, which is a strongly typed columnar storage format that can easily be read and written by <a href="https://spark.apache.org/docs/latest/index.html">Spark</a>, as well as being compatible with SQL interfaces such as <a href="https://cwiki.apache.org/confluence/display/Hive/Home">Hive</a> and <a href="http://prestosql.io/">Presto</a>. Some data is also stored in <a href="https://hekad.readthedocs.io/en/dev/message/index.html#stream-framing">Heka-framed protobuf</a> format. This custom format is usually reserved for data where we do not have a complete <a href="https://github.com/mozilla-services/mozilla-pipeline-schemas">JSONSchema specification</a>.</p>
<p>Using S3 for storage avoids the need for an always-on cluster, which means that data at rest is inexpensive. S3 also makes it very easy to automatically expire (delete) objects after a certain period of time, which is helpful for implementing data retention policies.</p>
<p>Once written to S3, the data is typically treated as immutable - data is not appended to existing files, nor is data normally updated in place. The exception here is when data is back-filled, in which case previous data may be overwritten.</p>
<p>There are a number of other types of storage used for more specialized applications, including relational databases (such as PostgreSQL for the <a href="https://github.com/mozilla/python_mozaggregator/#api">Telemetry Aggregates</a>) and NoSQL databases (DynamoDB is used for a backing store for the <a href="https://github.com/mozilla/python_mozetl/blob/master/mozetl/taar/taar_dynamo.py">TAAR project</a>). Reading data from a variety of RDBMS sources is also supported via STMO.</p>
<p>The data stored in Heka format is <a href="../../tools/spark.html">readable from Spark</a> using libraries in <a href="https://github.com/mozilla/moztelemetry/blob/master/src/main/scala/com/mozilla/telemetry/heka/Dataset.scala">Scala</a> or <a href="https://mozilla.github.io/python_moztelemetry/api.html#dataset">Python</a>.</p>
<p>Parquet data can be read and written natively from Spark, and many datasets are indexed in a <a href="https://cwiki.apache.org/confluence/display/Hive/Home">Hive</a> Metastore, making them available through a SQL interface on STMO and in notebooks via Spark SQL. Many other SQL data sources are also made available via STMO, see <a href="../../tools/stmo.html">this article</a> for more information on accessing data using SQL.</p>
<p>There is a separate data store for self-serve <strong>Analysis Outputs</strong>, intended to keep ad-hoc, temporary data out of the Data Lake. This is implemented as a separate S3 location, with personal output locations prefixed with each person's user id, similar to the layout of the <code>/home</code> directory on a Unix system.</p>
<p>Analysis outputs can also be made public using the <strong>Public Outputs</strong> bucket. This is a web-accessible S3 location for powering public dashboards. This public data is available at <code>https://analysis-output.telemetry.mozilla.org/&lt;job name&gt;/data/&lt;files&gt;</code>.</p>
<h2><a class="header" href="#stream-processing" id="stream-processing">Stream Processing</a></h2>
<p>Stream processing is done using <a href="https://github.com/mozilla-services/hindsight">Hindsight</a> and <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Spark Streaming</a>.</p>
<p>Hindsight allows you to run <a href="http://mozilla-services.github.io/lua_sandbox/">plugins written in Lua inside a sandbox</a>. This gives a safe, performant way to do self-serve streaming analysis. Hindsight plugins do the initial data validation and decoding, as well as writing out to long-term storage in both <a href="https://hekad.readthedocs.io/en/dev/message/index.html#stream-framing">Heka-framed protobuf</a> and <a href="https://mozilla-services.github.io/lua_sandbox_extensions/parquet/">parquet</a> forms.</p>
<p>Spark Streaming is used to read from Kafka and perform <a href="https://github.com/mozilla/telemetry-streaming">low-latency ETL and aggregation tasks</a>. These aggregates are currently used by <a href="https://data-missioncontrol.dev.mozaws.net">Mission Control</a> and are also available for querying via <a href="../../tools/stmo.html">STMO</a>.</p>
<h2><a class="header" href="#batch-processing" id="batch-processing">Batch Processing</a></h2>
<p>Batch processing is done using <a href="https://spark.apache.org/docs/latest/index.html">Spark</a>. Production ETL code is written in both <a href="https://github.com/mozilla/python_mozetl">Python</a> and <a href="https://github.com/mozilla/telemetry-batch-view">Scala</a>.</p>
<p>There are <a href="https://mozilla.github.io/python_moztelemetry/api.html#dataset">Python</a> and <a href="https://github.com/mozilla/moztelemetry/blob/master/src/main/scala/com/mozilla/telemetry/heka/Dataset.scala">Scala</a> libraries for reading data from the Data Lake in <a href="https://hekad.readthedocs.io/en/dev/message/index.html#stream-framing">Heka-framed protobuf</a> form, though it is much easier and more performant to make use of a <a href="../../concepts/choosing_a_dataset.html">derived dataset</a> whenever possible.</p>
<p>Datasets in parquet format can be read natively by Spark, either using Spark SQL or by reading data directly from S3.</p>
<p>Data produced by production jobs go into the Data Lake, while output from ad-hoc jobs go into Analysis Outputs.</p>
<p>Job scheduling and dependency management is done using <a href="https://github.com/mozilla/telemetry-airflow">Airflow</a>. Most jobs run once a day, processing data from &quot;yesterday&quot; on each run. A typical job launches a cluster, which fetches the specified ETL code as part of its bootstrap on startup, runs the ETL code, then shuts down upon completion. If something goes wrong, a job may time out or fail, and in this case it is retried automatically.</p>
<h2><a class="header" href="#self-serve-data-analysis" id="self-serve-data-analysis">Self Serve Data Analysis</a></h2>
<pre class="mermaid">graph TD
  subgraph Storage
    lake[Data Lake]
    s3_output_public[Public Outputs]
    s3_output_private[Analysis Outputs]
  end
  subgraph STMO
    STMO[STMO] --&gt;|read| lake
  end
  subgraph TMO
    evo[Evolution Dashboard]
    histo[Histogram Dashboard]
    agg[Telemetry Aggregates]
    evo -.- agg
    histo -.- agg
  end
  subgraph Databricks
    db_notebook[Notebook]
    db_notebook --&gt;|read + write| lake
  end
</pre>
<p>Most of the data analysis tooling has been developed with the goal of being &quot;self-serve&quot;. This means that people should be able to access and analyze data on their own, without involving data engineers or operations. Thus can data access scale beyond a small set of people with specialized knowledge of the entire pipeline.</p>
<p>The use of these self-serve tools is described in the <a href="../../concepts/analysis_intro.html">Getting Started</a> article. This section focuses on how these tools integrate with the platform infrastructure.</p>
<h5><a class="header" href="#stmo-sql-analysis" id="stmo-sql-analysis">STMO: SQL Analysis</a></h5>
<p><a href="../../tools/stmo.html">STMO</a> is a customized <a href="https://redash.io">Redash</a> installation that provides self-serve access to a a variety of different <a href="../../concepts/choosing_a_dataset.html">datasets</a>. From here, you can query data in the Parquet Data Lake as well as various RDBMS data sources.</p>
<p>STMO interfaces with the data lake using both <a href="http://prestosql.io/">Presto</a> and Amazon <a href="https://aws.amazon.com/athena/">Athena</a>. Each has its own data source in STMO. Since Athena does not support user-defined functions, datasets with HyperLogLog columns, such as <a href="../../datasets/obsolete/client_count_daily/reference.html"><code>client_count_daily</code></a>, are only available via Presto..</p>
<p>Different <strong>Data Sources</strong> in STMO connect to different backends, and each backend might use a slightly different flavor of SQL. You should find a link to the documentation for the expected SQL variant next to the Data Sources list.</p>
<p>Queries can be run just once, or scheduled to run periodically to keep data up-to-date.</p>
<p>There is a command-line interface to STMO called <a href="https://github.com/mozilla/stmocli">St. Mocli</a>, if you prefer writing SQL using your own editor and tools.</p>
<h5><a class="header" href="#databricks-managed-spark-analysis" id="databricks-managed-spark-analysis">Databricks: Managed Spark Analysis</a></h5>
<p>Our Databricks instance (see <a href="https://docs.databricks.com/user-guide/notebooks/index.html">Databricks docs</a>) offers another notebook interface for doing analysis in Scala, SQL, Python and R.</p>
<p>Databricks provides an always-on shared server which is nice for quick data investigations.</p>
<h5><a class="header" href="#tmo-aggregate-graphs" id="tmo-aggregate-graphs">TMO: Aggregate Graphs</a></h5>
<p><a href="https://telemetry.mozilla.org">TMO</a> provides easy visualizations of histogram and scalar measures over time. Time can be in terms of either builds or submission dates. This is the most convenient interface to the Telemetry data, as it does not require any custom code.</p>
<h2><a class="header" href="#visualization" id="visualization">Visualization</a></h2>
<p>There are a number of visualization libraries and tools being used to display data.</p>
<h5><a class="header" href="#tmo-dashboards" id="tmo-dashboards">TMO Dashboards</a></h5>
<p>The landing page at <a href="https://telemetry.mozilla.org"><code>telemetry.mozilla.org</code></a> is a good place to look for existing graphs, notably the <a href="https://telemetry.mozilla.org/new-pipeline/dist.html">measurement dashboard</a> which gives a lot of information about histogram and scalar measures collected on pre-release channels.</p>
<h5><a class="header" href="#notebooks" id="notebooks">Notebooks</a></h5>
<p>Use of interactive notebooks has become a standard in the industry, and Mozilla makes heavy use of this approach. Databricks makes it easy to run, share, and schedule notebooks.</p>
<h5><a class="header" href="#others" id="others">Others</a></h5>
<p><a href="../../tools/stmo.html">STMO</a> lets you query the data using SQL, but it also supports a number of useful visualizations.</p>
<p><a href="BROKEN:http://pipeline-cep.prod.mozaws.net/">Hindsight's web interface</a> has the ability to visualize time-series data.</p>
<p><a href="https://data-missioncontrol.dev.mozaws.net">Mission Control</a> gives a low-latency view into release health.</p>
<p>Many bespoke visualizations are built using the <a href="http://metricsgraphicsjs.org/">Metrics Graphics</a> library as a display layer.</p>
<h2><a class="header" href="#monitoring-and-alerting" id="monitoring-and-alerting">Monitoring and Alerting</a></h2>
<p>There are multiple layers of monitoring and alerting.</p>
<p>At a low level, the system is monitored to ensure that it is functioning as expected. This includes things like machine-level resources (network capacity, disk space, available RAM, CPU load) which are typically monitored using <a href="http://datadoghq.com/">DataDog</a>.</p>
<p>Next, we monitor the &quot;transport&quot; functionality of the system. This includes monitoring incoming submission rates, payload sizes, traffic patterns, schema validation failure rates, and alerting if anomalies are detected. This type of anomaly detection and alerting is handled by <a href="https://github.com/mozilla-services/hindsight">Hindsight</a>.</p>
<p>Once data has been safely ingested and stored, we run some automatic regression detection on all Telemetry <a href="https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/collection/histograms.html">histogram measures</a> using <a href="https://github.com/mozilla/cerberus">Cerberus</a>. This code looks for changes in the distribution of a measure, and emails probe owners if a significant change is observed.</p>
<p>Production ETL jobs are run via <a href="https://github.com/mozilla/telemetry-airflow">Airflow</a>, which monitors batch job progress and alerts if there are failures in any job. Self-serve batch jobs running via Databricks also generate alerts upon failure.</p>
<p>Scheduled <a href="../../tools/stmo.html">STMO</a> queries may also be configured to generate alerts, which is used to monitor the last-mile user facing status of derived datasets. STMO may also be used to monitor and alert on high-level characteristics of the data, or really anything you can think of.</p>
<h2><a class="header" href="#data-exports" id="data-exports">Data Exports</a></h2>
<p>Data is exported from the pipeline to a few other tools and systems. Examples include integration with <a href="https://amplitude.com/">Amplitude</a> for mobile and product analytics and shipping data to other parts of the Mozilla organization.</p>
<p>There are also a few data sets which are made publicly available, such as the <a href="https://data.firefox.com/dashboard/hardware">Firefox Hardware Report</a>.</p>
<h2><a class="header" href="#bringing-it-all-together" id="bringing-it-all-together">Bringing it all together</a></h2>
<p>Finally, here is a more detailed view of the entire platform. Some connections are omitted for clarity.</p>
<pre class="mermaid">graph LR
 subgraph Data Producers
  Firefox
  more_producers[...]
 end
 subgraph Storage
  Landfill
  warehouse_heka[Heka Data Lake]
  warehouse_parquet[Parquet Data Lake]
  warehouse_analysis[Analysis Outputs]
  PostgreSQL
  Redshift
  MySQL
  hive[Hive] -.- warehouse_parquet
 end
 subgraph Stream Processing
  cep[Hindsight Streaming]
  dwl[Data Store Loader] --&gt; warehouse_heka
  dwl --&gt; warehouse_parquet
  sparkstreaming[Spark Streaming] --&gt; warehouse_parquet
 end
 subgraph Ingestion
  Firefox --&gt; lb[Load Balancer]
  more_producers --&gt; lb
  lb --&gt; tee
  tee --&gt; mozingest
  mozingest --&gt; kafka
  mozingest --&gt; Landfill
  ZooKeeper -.- kafka[Kafka]
  kafka --&gt; dwl
  kafka --&gt; cep
  kafka --&gt; sparkstreaming
 end
 subgraph Batch Processing
  Airflow -.-&gt;|spark|tbv[telemetry-batch-view]
  Airflow -.-&gt;|spark|python_mozetl
  warehouse_heka --&gt; tbv
  warehouse_parquet --&gt; tbv
  warehouse_heka --&gt; python_mozetl
  warehouse_parquet --&gt; python_mozetl
  tmo_agg[Telemetry Aggregates]
 end
 subgraph Visualization
  Hindsight
  Jupyter
  Zeppelin
  TMO
  redash_graphs[STMO]
  MissionControl
  bespoke_viz[Bespoke Viz]
 end
 subgraph Export
  tbv --&gt; Amplitude
  sparkstreaming --&gt; Amplitude
 end
 subgraph Self Serve
  redash[STMO] -.-&gt; Presto
  Presto --&gt; hive
  redash -.-&gt; Athena
  Athena --&gt; hive
  warehouse_heka --&gt; spcluster
  warehouse_parquet --&gt; spcluster
  spcluster --&gt; warehouse_analysis
 end
 Schemas -.-&gt;|validation| dwl
</pre>
<footer id="open-on-gh">Found a bug? <a href="https://github.com/mozilla/data-docs/edit/master/src/concepts/pipeline/data_pipeline_detail.md">Edit this file on GitHub.</a></footer>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../../concepts/pipeline/data_pipeline.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../../concepts/censuses.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../../concepts/pipeline/data_pipeline.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="../../concepts/censuses.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        
        <!-- Google Analytics Tag -->
        <script type="text/javascript">
            var localAddrs = ["localhost", "127.0.0.1", ""];

            // make sure we don't activate google analytics if the developer is
            // inspecting the book locally...
            if (localAddrs.indexOf(document.location.hostname) === -1) {
                (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

                ga('create', 'UA-104326577-1', 'auto');
                ga('send', 'pageview');
            }
        </script>
        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
        <script type="text/javascript" src="../../mermaid.min.js"></script>
        
        <script type="text/javascript" src="../../mermaid-init.js"></script>
        

        

    </body>
</html>
